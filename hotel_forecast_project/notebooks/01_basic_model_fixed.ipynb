{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 酒店选址与客流预测基础模型 (修复版)\n",
    "\n",
    "本笔记本演示酒店选址和客流预测的基础模型实现。\n",
    "\n",
    "## 注意：模型更新\n",
    "\n",
    "原始模型在处理没有节点索引的数据时出现错误：\n",
    "```\n",
    "TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n",
    "```\n",
    "\n",
    "我们已修复`SpatialTemporalDataset`类和`SpatialTemporalModel.forward`方法，使其能够正确处理没有节点索引的情况。修复包括：\n",
    "\n",
    "1. 将`None`值替换为空张量\n",
    "2. 在模型的`forward`方法中添加适当的检查\n",
    "\n",
    "如果您遇到这个错误，请确保使用最新版本的模型文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e01c6a39b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "# 添加项目根目录到路径\n",
    "sys.path.append('..')\n",
    "\n",
    "# 导入自定义模块前确保重新加载以使用最新版本\n",
    "if 'src.models.spatial_temporal_model' in sys.modules:\n",
    "    importlib.reload(sys.modules['src.models.spatial_temporal_model'])\n",
    "\n",
    "# 导入自定义模块\n",
    "from src.models.spatial_temporal_model import SpatialTemporalModel, SpatialTemporalTrainer, SpatialTemporalDataset, prepare_sequence_data\n",
    "from src.models.location_selection_model import LocationSelectionModel\n",
    "from src.utils.data_collection import download_hotel_booking_data, fetch_poi_data\n",
    "from src.utils.data_preprocessing import preprocess_hotel_data, create_features, integrate_poi_data\n",
    "\n",
    "# 设置绘图样式\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 加载原始数据（可选）\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     hotel_df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m'\u001b[39m\u001b[33m../data/raw/hotel-booking-demand/hotel_bookings.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m原始数据加载成功！形状: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhotel_df.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# 加载原始数据（可选）\n",
    "try:\n",
    "    hotel_df = pd.read_csv('../data/raw/hotel-booking-demand/hotel_bookings.csv')\n",
    "    print(f\"原始数据加载成功！形状: {hotel_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"原始数据文件不存在，尝试下载数据...\")\n",
    "    try:\n",
    "        # 尝试从项目根目录导入下载函数\n",
    "        from download_data import download_hotel_data\n",
    "        download_success = download_hotel_data()\n",
    "        if download_success:\n",
    "            print(\"数据下载成功，正在加载...\")\n",
    "            hotel_df = pd.read_csv('../data/raw/hotel-booking-demand/hotel_bookings.csv')\n",
    "            print(f\"原始数据加载成功！形状: {hotel_df.shape}\")\n",
    "        else:\n",
    "            print(\"数据下载失败，请手动下载数据\")\n",
    "            hotel_df = pd.DataFrame()  # 创建空DataFrame\n",
    "    except (ImportError, ModuleNotFoundError):\n",
    "        print(\"未找到下载模块，请手动下载数据或运行项目根目录的download_data.py\")\n",
    "        hotel_df = pd.DataFrame()  # 创建空DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理数据加载成功！形状: (5000, 43)\n"
     ]
    }
   ],
   "source": [
    "# 直接加载预处理后的数据\n",
    "try:\n",
    "    processed_df = pd.read_csv('../data/processed/hotel_processed.csv')\n",
    "    print(f\"预处理数据加载成功！形状: {processed_df.shape}\")\n",
    "    processed_df.head()\n",
    "except FileNotFoundError:\n",
    "    print(\"预处理数据文件不存在，请先运行fix_preprocessing.py脚本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征数据加载成功！形状: (5000, 63)\n"
     ]
    }
   ],
   "source": [
    "# 加载特征工程后的数据\n",
    "try:\n",
    "    features_df = pd.read_csv('../data/processed/hotel_features.csv')\n",
    "    print(f\"特征数据加载成功！形状: {features_df.shape}\")\n",
    "    features_df.head()\n",
    "except FileNotFoundError:\n",
    "    print(\"特征数据文件不存在，请先运行fix_preprocessing.py脚本\")\n",
    "    # 如果预处理数据存在但特征数据不存在，可以创建特征\n",
    "    if 'processed_df' in locals() and not processed_df.empty:\n",
    "        print(\"从预处理数据创建特征...\")\n",
    "        features_df = create_features(processed_df)\n",
    "        # 保存处理后的数据\n",
    "        os.makedirs('../data/processed', exist_ok=True)\n",
    "        features_df.to_csv('../data/processed/hotel_features.csv', index=False)\n",
    "        print(\"特征数据已保存到 ../data/processed/hotel_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 空间时间序列模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "序列数据形状: X=(4994, 7, 12), y=(4994, 1)\n",
      "训练集: X=(3995, 7, 12), y=(3995, 1)\n",
      "测试集: X=(999, 7, 12), y=(999, 1)\n"
     ]
    }
   ],
   "source": [
    "# 准备时间序列数据\n",
    "if 'features_df' in locals() and not features_df.empty:\n",
    "    # 选择特征列和目标列\n",
    "    # 注意: 实际列名应根据实际数据调整\n",
    "    feature_cols = ['lead_time', 'arrival_date_month_num', 'stays_in_weekend_nights', \n",
    "                     'stays_in_week_nights', 'adults', 'children', 'is_repeated_guest', \n",
    "                     'previous_cancellations', 'previous_bookings_not_canceled', \n",
    "                     'booking_changes', 'required_car_parking_spaces', 'total_of_special_requests']\n",
    "    \n",
    "    # 确保所有特征列都存在\n",
    "    for col in feature_cols:\n",
    "        if col not in features_df.columns:\n",
    "            print(f\"警告：列 {col} 不在特征数据中\")\n",
    "    \n",
    "    # 只使用存在的特征列\n",
    "    available_feature_cols = [col for col in feature_cols if col in features_df.columns]\n",
    "    \n",
    "    target_cols = ['adr']  # 平均每日房价\n",
    "    \n",
    "    # 准备序列数据\n",
    "    X, y, _ = prepare_sequence_data(features_df, available_feature_cols, target_cols, seq_len=7, stride=1)\n",
    "    \n",
    "    print(f\"序列数据形状: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    # 划分训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"训练集: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"测试集: X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据批次数: 125\n",
      "测试数据批次数: 32\n"
     ]
    }
   ],
   "source": [
    "# 创建数据集和数据加载器\n",
    "if 'X_train' in locals():\n",
    "    batch_size = 32\n",
    "    \n",
    "    train_dataset = SpatialTemporalDataset(X_train, y_train)\n",
    "    test_dataset = SpatialTemporalDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"训练数据批次数: {len(train_loader)}\")\n",
    "    print(f\"测试数据批次数: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 21:43:09,120 - src.models.spatial_temporal_model - INFO - 模型初始化完成，使用设备: cpu\n",
      "2025-04-19 21:43:09,121 - src.models.spatial_temporal_model - INFO - 开始训练，总epochs: 5\n",
      "2025-04-19 21:43:09,152 - src.models.spatial_temporal_model - INFO - 批次 0/125, 损失: nan\n",
      "2025-04-19 21:43:09,278 - src.models.spatial_temporal_model - INFO - 批次 10/125, 损失: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练空间时间模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 21:43:09,390 - src.models.spatial_temporal_model - INFO - 批次 20/125, 损失: nan\n",
      "2025-04-19 21:43:09,480 - src.models.spatial_temporal_model - INFO - 批次 30/125, 损失: nan\n",
      "2025-04-19 21:43:09,576 - src.models.spatial_temporal_model - INFO - 批次 40/125, 损失: nan\n",
      "2025-04-19 21:43:09,669 - src.models.spatial_temporal_model - INFO - 批次 50/125, 损失: nan\n",
      "2025-04-19 21:43:09,761 - src.models.spatial_temporal_model - INFO - 批次 60/125, 损失: nan\n",
      "2025-04-19 21:43:09,858 - src.models.spatial_temporal_model - INFO - 批次 70/125, 损失: nan\n",
      "2025-04-19 21:43:09,950 - src.models.spatial_temporal_model - INFO - 批次 80/125, 损失: nan\n",
      "2025-04-19 21:43:10,059 - src.models.spatial_temporal_model - INFO - 批次 90/125, 损失: nan\n",
      "2025-04-19 21:43:10,152 - src.models.spatial_temporal_model - INFO - 批次 100/125, 损失: nan\n",
      "2025-04-19 21:43:10,252 - src.models.spatial_temporal_model - INFO - 批次 110/125, 损失: nan\n",
      "2025-04-19 21:43:10,347 - src.models.spatial_temporal_model - INFO - 批次 120/125, 损失: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m开始训练空间时间模型...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 注意: 实际训练时应使用更多的epoch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m history = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\cyg\\统计建模new\\hotel_forecast_project\\notebooks\\..\\src\\models\\spatial_temporal_model.py:333\u001b[39m, in \u001b[36mSpatialTemporalTrainer.train\u001b[39m\u001b[34m(self, train_dataloader, valid_dataloader, epochs, adj, patience, model_path)\u001b[39m\n\u001b[32m    330\u001b[39m train_losses.append(train_loss)\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# 验证\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m valid_loss = metrics[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    335\u001b[39m valid_losses.append(valid_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\cyg\\统计建模new\\hotel_forecast_project\\notebooks\\..\\src\\models\\spatial_temporal_model.py:299\u001b[39m, in \u001b[36mSpatialTemporalTrainer.validate\u001b[39m\u001b[34m(self, dataloader, adj)\u001b[39m\n\u001b[32m    296\u001b[39m all_preds = np.concatenate(all_preds, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    297\u001b[39m all_targets = np.concatenate(all_targets, axis=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m mae = \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n\u001b[32m    301\u001b[39m r2 = r2_score(all_targets, all_preds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_regression.py:277\u001b[39m, in \u001b[36mmean_absolute_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[32m    223\u001b[39m \n\u001b[32m    224\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    272\u001b[39m \u001b[33;03m0.85...\u001b[39;00m\n\u001b[32m    273\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    274\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    276\u001b[39m _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m )\n\u001b[32m    282\u001b[39m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[32m    284\u001b[39m output_errors = _average(\n\u001b[32m    285\u001b[39m     xp.abs(y_pred - y_true), weights=sample_weight, axis=\u001b[32m0\u001b[39m, xp=xp\n\u001b[32m    286\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_regression.py:198\u001b[39m, in \u001b[36m_check_reg_targets_with_floating_dtype\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Ensures that y_true, y_pred, and sample_weight correspond to the same\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[33;03mregression task.\u001b[39;00m\n\u001b[32m    150\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    196\u001b[39m dtype_name = _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m y_type, y_true, y_pred, multioutput = \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# _check_reg_targets does not accept sample_weight as input.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# Convert sample_weight's data type separately to match dtype_name.\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_regression.py:106\u001b[39m, in \u001b[36m_check_reg_targets\u001b[39m\u001b[34m(y_true, y_pred, multioutput, dtype, xp)\u001b[39m\n\u001b[32m    104\u001b[39m check_consistent_length(y_true, y_pred)\n\u001b[32m    105\u001b[39m y_true = check_array(y_true, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m y_pred = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_true.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    109\u001b[39m     y_true = xp.reshape(y_true, (-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# 创建并训练模型\n",
    "if 'train_loader' in locals():\n",
    "    # 模型参数\n",
    "    input_dim = X_train.shape[2]  # 特征维度\n",
    "    hidden_dim = 64\n",
    "    output_dim = y_train.shape[1]  # 目标维度\n",
    "    num_nodes = 100  # 空间节点数量\n",
    "    seq_len = X_train.shape[1]  # 序列长度\n",
    "    \n",
    "    # 创建模型\n",
    "    model = SpatialTemporalModel(\n",
    "        input_dim=input_dim, \n",
    "        hidden_dim=hidden_dim, \n",
    "        output_dim=output_dim, \n",
    "        num_nodes=num_nodes, \n",
    "        seq_len=seq_len\n",
    "    )\n",
    "    \n",
    "    # 创建训练器\n",
    "    trainer = SpatialTemporalTrainer(model, learning_rate=0.001)\n",
    "    \n",
    "    # 训练模型\n",
    "    print(\"开始训练空间时间模型...\")\n",
    "    # 注意: 实际训练时应使用更多的epoch\n",
    "    history = trainer.train(train_loader, test_loader, epochs=5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练历史\n",
    "if 'history' in locals():\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_losses'], label='训练损失')\n",
    "    plt.plot(history['valid_losses'], label='验证损失')\n",
    "    plt.title('训练过程中的损失')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('损失')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    metrics = [m['rmse'] for m in history['valid_metrics']]\n",
    "    plt.plot(metrics, label='验证RMSE')\n",
    "    plt.title('验证集RMSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 选址评分模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建位置特征数据集\n",
    "# 注意: 实际应用中需要真实的位置数据\n",
    "# 这里仅为演示，创建一些假数据\n",
    "\n",
    "def create_dummy_location_data(n_locations=50):\n",
    "    \"\"\"创建模拟位置数据\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    location_data = {\n",
    "        'location_id': [f'LOC_{i:03d}' for i in range(n_locations)],\n",
    "        'latitude': np.random.uniform(31.1, 31.3, n_locations),\n",
    "        'longitude': np.random.uniform(121.4, 121.6, n_locations),\n",
    "        'poi_restaurant_count': np.random.randint(5, 50, n_locations),\n",
    "        'poi_shopping_count': np.random.randint(3, 30, n_locations),\n",
    "        'poi_entertainment_count': np.random.randint(2, 20, n_locations),\n",
    "        'poi_transport_count': np.random.randint(1, 15, n_locations),\n",
    "        'distance_to_city_center': np.random.uniform(0.5, 15.0, n_locations),\n",
    "        'distance_to_airport': np.random.uniform(5.0, 50.0, n_locations),\n",
    "        'distance_to_subway': np.random.uniform(0.1, 3.0, n_locations),\n",
    "        'population_density': np.random.uniform(5000, 25000, n_locations),\n",
    "        'income_per_capita': np.random.uniform(80000, 150000, n_locations),\n",
    "        'unemployment_rate': np.random.uniform(2.0, 8.0, n_locations),\n",
    "        'competitor_count': np.random.randint(0, 10, n_locations),\n",
    "        'area_km2': np.random.uniform(0.5, 5.0, n_locations)\n",
    "    }\n",
    "    \n",
    "    # 添加一个模拟目标值(可以视为历史业绩)\n",
    "    # 假设业绩与多个因素有关，并添加一些随机噪声\n",
    "    score = (\n",
    "        0.3 * location_data['poi_restaurant_count'] + \n",
    "        0.2 * location_data['poi_shopping_count'] + \n",
    "        0.2 * location_data['poi_entertainment_count'] + \n",
    "        0.3 * location_data['poi_transport_count'] - \n",
    "        0.5 * location_data['distance_to_city_center'] - \n",
    "        0.2 * location_data['distance_to_subway'] + \n",
    "        0.1 * location_data['population_density'] / 1000 + \n",
    "        0.1 * location_data['income_per_capita'] / 10000 - \n",
    "        0.3 * location_data['competitor_count'] + \n",
    "        np.random.normal(0, 5, n_locations)\n",
    "    )\n",
    "    # 转换为正值并归一化到0-100范围\n",
    "    score = score - min(score)\n",
    "    score = 100 * score / max(score)\n",
    "    location_data['historical_score'] = score\n",
    "    \n",
    "    return pd.DataFrame(location_data)\n",
    "\n",
    "# 创建位置数据\n",
    "location_df = create_dummy_location_data(100)\n",
    "print(f\"生成模拟位置数据: {location_df.shape}\")\n",
    "location_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备选址模型数据\n",
    "feature_cols = [\n",
    "    'poi_restaurant_count', 'poi_shopping_count', 'poi_entertainment_count', 'poi_transport_count',\n",
    "    'distance_to_city_center', 'distance_to_airport', 'distance_to_subway',\n",
    "    'population_density', 'income_per_capita', 'unemployment_rate', 'competitor_count', 'area_km2'\n",
    "]\n",
    "\n",
    "X = location_df[feature_cols].values\n",
    "y = location_df['historical_score'].values\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"训练集: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"测试集: X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练选址评分模型\n",
    "print(\"开始训练选址评分模型...\")\n",
    "\n",
    "# 使用XGBoost模型\n",
    "location_model = LocationSelectionModel(model_type='xgboost')\n",
    "location_model.fit(X_train, y_train, feature_names=feature_cols)\n",
    "\n",
    "# 在测试集上评估\n",
    "y_pred = location_model.predict(X_test)\n",
    "\n",
    "# 计算指标\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"选址模型评估指标:\")\n",
    "print(f\"MSE:  {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看特征重要性\n",
    "feature_importance = location_model.get_feature_importance(plot=True)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为候选位置评分（使用测试集作为候选位置）\n",
    "test_df = location_df.iloc[list(np.random.choice(len(location_df), 20, replace=False))].copy()\n",
    "scored_locations = location_model.score_locations(test_df, feature_cols)\n",
    "\n",
    "print(\"候选位置评分结果:\")\n",
    "scored_locations[['location_id', 'predicted_score'] + feature_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化候选位置得分\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='location_id', y='predicted_score', data=scored_locations.head(10))\n",
    "plt.title('前10个候选位置得分')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型目录\n",
    "os.makedirs('../data/models', exist_ok=True)\n",
    "\n",
    "# 保存选址模型\n",
    "if 'location_model' in locals():\n",
    "    location_model.save_model('../data/models/location_model.pkl')\n",
    "    print(\"选址模型已保存到 ../data/models/location_model.pkl\")\n",
    "\n",
    "# 保存空间时间模型\n",
    "if 'trainer' in locals() and 'model' in locals():\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'input_dim': input_dim,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'output_dim': output_dim,\n",
    "        'num_nodes': num_nodes,\n",
    "        'seq_len': seq_len\n",
    "    }, '../data/models/spatial_temporal_model.pt')\n",
    "    print(\"空间时间模型已保存到 ../data/models/spatial_temporal_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 总结\n",
    "\n",
    "在本笔记本中，我们实现了酒店选址与客流预测的两个基础模型：\n",
    "\n",
    "1. **空间时间注意力机制模型**：用于预测酒店入住率和每日平均房价，结合了空间和时间两个维度的信息。\n",
    "2. **选址评分模型**：使用XGBoost算法，基于位置特征为候选地址打分，帮助酒店选择最佳位置。\n",
    "\n",
    "这些模型可以进一步优化和扩展：\n",
    "\n",
    "- 收集更多真实数据，特别是位置相关的POI数据和历史入住率数据\n",
    "- 优化模型超参数\n",
    "- 增加更多特征，如季节性特征、天气数据、事件数据等\n",
    "- 实现冷启动策略，处理新酒店没有历史数据的情况\n",
    "- 部署模型并提供API服务"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
